{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I used TFAutoModelForCasualLM and AutoTokenizer to automatically load the correct model based on a specific checkpoint. A checkpoint contains the weights of a pre-trained model.\n",
    "\n",
    "In this case, I imported the DistilGPT-2 checkpoint. I also set the end-of-sequence token as a padding token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, AdamWeightDecay, pipeline, create_optimizer\n",
    "from transformers import DefaultDataCollator\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 762/762 [00:00<00:00, 281kB/s]\n",
      "Downloading: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.56MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 1.18MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.09MB/s]\n",
      "Downloading: 100%|██████████| 328M/328M [00:23<00:00, 14.2MB/s] \n",
      "2023-01-29 22:20:40.264872: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-29 22:20:40.264948: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.1-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.11.0\n",
      "Keras Version: 2.11.0\n",
      "\n",
      "Python 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.1\n",
      "SciPy 1.10.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('data/publications/final_database_of_papers.xlsx',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.140000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.461392e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.953457e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.928938e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.358038e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.529863e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.624275e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.656490e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         article_id\n",
       "count  3.140000e+03\n",
       "mean   3.461392e+07\n",
       "std    1.953457e+06\n",
       "min    2.928938e+07\n",
       "25%    3.358038e+07\n",
       "50%    3.529863e+07\n",
       "75%    3.624275e+07\n",
       "max    3.656490e+07"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company_name', 'article_id', 'title', 'keywords', 'publication_date',\n",
       "       'abstract', 'journal', 'doi', 'keyword_display'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3140 entries, 0 to 3139\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   company_name      3140 non-null   object        \n",
      " 1   article_id        3140 non-null   int64         \n",
      " 2   title             3140 non-null   object        \n",
      " 3   keywords          3140 non-null   object        \n",
      " 4   publication_date  3140 non-null   datetime64[ns]\n",
      " 5   abstract          3140 non-null   object        \n",
      " 6   journal           3140 non-null   object        \n",
      " 7   doi               3135 non-null   object        \n",
      " 8   keyword_display   3140 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(7)\n",
      "memory usage: 245.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gather useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe[['abstract', 'title' , 'article_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" function to split df into train and test with sklearn train_test_split\"\"\"\n",
    "def split_train_test(df, test_size=0.2):\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    train, test = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gpt, test_gpt = split_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"0a77e2df-b55f-4f59-9475-0c456abd3660\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0a77e2df-b55f-4f59-9475-0c456abd3660\")) {                    Plotly.newPlot(                        \"0a77e2df-b55f-4f59-9475-0c456abd3660\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"variable=0<br>Article Length (words)=%{x}<br>count=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"nbinsx\":400,\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[165,44,142,252,291,71,276,202,273,187,75,219,251,188,201,193,270,103,188,181,288,82,204,160,142,393,231,147,247,218,42,13,243,39,247,164,156,18,128,111,143,32,227,273,206,154,215,45,182,77,132,246,68,238,327,109,113,197,214,255,211,102,349,275,225,200,275,166,150,61,71,215,237,81,88,95,303,153,91,108,296,292,96,196,29,246,51,148,185,187,295,210,298,246,44,196,244,225,241,171,97,22,55,152,75,204,192,400,147,118,118,80,155,203,184,289,277,329,149,233,155,256,104,247,235,78,41,245,214,250,134,270,102,121,274,142,245,109,196,199,301,22,147,137,234,31,36,305,58,165,17,168,126,181,220,266,44,150,155,202,253,146,291,200,258,209,267,248,163,173,150,252,246,113,150,158,297,243,99,95,75,19,258,236,120,155,242,178,36,245,208,255,136,130,300,151,266,18,248,18,219,205,261,250,223,248,172,262,268,189,439,201,291,148,151,100,224,160,197,184,144,32,69,161,150,207,244,51,240,100,151,35,179,78,131,192,65,158,170,51,373,193,168,241,121,79,358,182,264,176,255,121,190,169,93,145,170,203,100,67,172,229,157,292,234,210,162,168,122,222,192,152,243,70,195,123,49,142,199,151,254,271,88,250,393,261,120,141,50,315,229,300,171,319,191,175,209,84,352,348,381,15,158,137,134,132,235,136,158,28,115,160,203,27,199,123,85,203,226,234,161,215,347,31,205,158,174,90,278,225,152,172,270,265,192,146,268,151,46,218,243,190,200,247,163,180,289,68,263,241,286,244,159,61,21,47,246,179,169,241,143,192,74,66,250,179,227,18,210,161,95,31,252,135,178,235,243,152,148,204,188,116,25,193,128,234,226,348,260,45,84,30,143,217,201,223,261,200,69,165,27,271,206,193,296,13,158,189,187,233,130,143,83,28,102,169,178,233,14,188,153,151,133,293,249,157,39,142,197,78,38,85,155,175,55,242,161,44,345,87,126,257,163,21,261,272,157,120,154,243,151,124,214,221,234,162,242,249,269,202,168,195,196,355,161,240,114,246,255,173,189,248,119,131,147,266,476,432,249,353,246,147,200,242,173,159,128,282,166,154,12,535,13,153,150,172,40,197,217,126,125,148,218,124,147,15,123,185,145,151,174,191,199,155,155,144,211,73,146,121,151,49,178,225,52,150,182,185,154,170,188,268,160,160,329,405,297,244,189,207,145,147,147,220,140,204,258,119,128,163,177,150,201,72,180,245,127,19,74,142,154,168,134,169,348,267,209,194,181,201,193,225,184,229,254,98,169,174,79,565,180,246,120,137,153,52,26,21,105,288,202,251,202,64,109,316,89,90,217,430,42,200,203,298,154,236,236,247,405,194,195,302,244,219,355,200,250,253,170,182,255,261,196,123,30,186,250,376,191,211,371,17,321,233,292,247,203,222,234,299,294,306,245,130,94,260,220,265,342,34,194,99,196,459,248,187,150,285,232,246,111,48,222,241,298,106,283,248,53,149,72,295,239,179,248,219,72,175,220,170,217,270,158,235,289,203,211,204,186,227,250,296,47,190,205,217,341,254,153,176,418,64,151,226,244,188,232,61,273,239,213,143,251,260,530,95,124,301,46,215,118,150,253,200,207,84,118,79,252,199,201,191,498,213,116,233,243,114,137,218,189,195,246,52,56,123,184,200,147,212,58,54,130,266,170,186,90,149,38,75,244,80,49,144,114,166,188,186,238,182,140,151,10,18,98,119,199,192,231,163,133,181,78,126,50,221,37,191,219,237,225,228,195,281,191,177,293,76,199,62,299,134,193,248,33,26,52,256,71,82,287,197,25,194,74,222,253,231,67,217,130,148,43,205,152,198,206,170,197,126,150,274,136,89,151,150,199,10,200,173,228,296,247,302,257,171,87,238,247,261,246,192,26,131,123,308,185,224,303,93,201,252,259,234,222,212,172,53,54,113,315,109,107,377,234,150,204,285,126,246,216,228,297,150,200,329,258,314,48,156,182,364,143,155,236,195,314,198,174,241,280,45,39,139,294,274,218,268,146,177,229,122,142,23,31,346,315,207,215,168,242,215,346,44,26,255,112,368,289,162,96,207,223,197,15,164,261,241,61,297,204,350,102,91,168,436,121,112,269,168,80,253,26,198,196,335,131,115,146,528,135,228,355,197,172,58,127,110,253,102,480,212,243,123,169,217,244,64,281,20,253,231,161,590,241,239,207,173,143,227,105,85,247,148,150,259,179,205,138,205,146,254,198,140,207,33,71,170,155,196,175,146,97,116,52,227,198,187,257,67,188,202,166,147,152,152,161,269,187,191,238,155,211,136,67,185,246,138,148,167,259,153,245,222,67,235,175,194,187,405,126,174,1381,34,262,28,302,165,97,314,221,36,277,215,139,268,141,196,163,516,198,300,222,108,198,361,244,267,227,344,258,224,215,123,204,193,91,340,203,250,178,77,199,74,208,244,314,249,126,220,159,219,195,187,242,208,262,202,246,295,250,170,246,152,234,185,209,21,262,261,144,250,304,110,307,22,387,411,286,15,114,77,250,420,208,257,220,150,52,345,266,225,353,159,164,313,252,269,90,265,245,166,385,238,295,157,226,34,149,255,124,292,146,190,172,237,175,357,71,228,252,300,145,128,277,156,246,243,155,198,432,244,134,344,275,448,271,245,48,149,203,255,187,79,224,44,215,246,189,238,146,391,152,236,333,123,46,295,250,245,89,138,98,150,160,272,236,172,48,421,37,182,143,111,191,274,156,267,63,175,174,91,372,244,59,349,197,30,168,199,146,251,153,269,244,199,72,130,102,123,339,136,216,123,128,246,148,191,121,170,86,224,263,115,238,184,76,252,149,131,20,171,196,135,164,21,282,202,123,67,103,161,108,158,142,251,19,217,211,218,224,11,115,108,359,51,55,160,214,169,22,206,347,122,241,168,241,253,295,419,232,287,153,240,157,292,300,197,286,59,254,214,83,258,258,186,59,215,35,115,203,270,58,130,63,150,38,185,327,236,146,165,199,24,149,159,164,22,243,43,241,85,125,348,322,274,230,11,67,44,244,70,206,236,181,88,302,191,214,295,239,238,184,125,260,221,228,199,192,129,199,46,19,132,162,228,77,29,166,203,156,203,85,181,36,127,334,216,249,245,235,249,95,136,19,251,385,41,49,214,83,123,61,130,143,247,245,104,169,58,12,108,71,73,16,296,265,453,301,247,287,164,240,63,64,72,92,16,153,249,28,33,27,238,127,191,326,251,86,87,24,199,342,240,168,361,400,402,140,41,243,257,51,66,352,45,335,205,160,151,136,236,248,261,21,239,209,217,139,367,313,38,123,252,56,134,44,304,43,169,155,356,171,50,196,24,246,248,59,144,100,279,95,88,200,183,157,196,132,249,247,26,46,267,207,226,11,139,224,377,326,180,204,159,45,79,88,24,192,127,177,13,27,149,271,180,70,43,356,234,259,98,188,292,330,145,252,117,95,198,196,213,167,39,35,70,151,126,201,65,183,366,150,248,193,232,188,165,67,60,281,190,155,266,266,91,218,351,241,15,288,137,225,59,176,202,202,269,13,188,184,182,56,210,225,195,255,261,260,126,70,127,185,200,190,356,49,18,271,180,28,198,63,129,149,51,35,178,141,55,150,10,250,214,183,249,56,251,62,200,231,199,167,229,154,236,203,167,139,297,158,49,231,274,221,121,192,85,88,31,250,188,132,310,250,146,208,210,218,346,336,200,283,79,262,308,176,186,301,98,150,72,86,352,21,12,358,23,41,136,223,270,244,306,201,151,120,195,175,184,146,227,54,200,133,200,241,24,17,31,49,74,253,57,73,256,247,58,55,230,39,198,202,248,351,251,149,198,12,191,198,214,148,208,32,151,195,192,346,142,255,176,112,143,96,64,270,300,451,73,124,236,235,72,115,121,74,39,235,251,69,11,128,162,178,149,22,284,178,116,298,147,300,536,220,207,152,213,159,226,235,20,254,21,283,21,222,117,247,133,36,124,196,295,154,195,186,259,264,236,170,150,211,163,112,256,245,160,234,190,259,137,257,128,150,212,18,121,402,231,146,241,91,275,88,181,18,202,250,310,125,273,294,245,424,172,208,284,36,157,250,304,246,209,175,179,247,340,358,469,384,105,63,270,83,321,399,338,209,324,237,175,463,216,485,84,254,202,264,422,234,84,149,342,475,392,471,213,33,30,200,136,131,258,249,249,65,214,277,31,196,130,230,496,485,615,206,405,356,198,147,212,304,138,368,25,198,164,320,456,181,278,197,188,153,103,353,523,456,285,336,284,42,170,260,239,203,342,406,371,261,348,329,25,125,576,79,545,239,148,451,179,540,337,375,97,195,25,98,288,27,425,47,67,117,444,105,334,353,614,338,558,441,434,188,217,71,107,628,170,324,161,330,41,237,469,28,338,42,24,34,224,408,12,56,296,185,521,107,201,101,92,204,310,231,252,157,39,215,137,223,224,441,201,139,335,238,22,10,159,42,33,166,289,208,372,425,184,207,379,189,344,476,276,51,51,254,349,85,211,326,59,235,105,357,260,14,250,193,91,93,129,433,332,316,402,479,221,162,260,126,129,105,201,146,377,136,117,159,193,245,266,375,398,102,318,109,199,336,290,331,277,285,172,147,78,242,214,119,163,297,382,87,381,27,458,144,313,205,139,389,124,364,318,150,159,66,49,86,166,46,194,228,14,127,42,60,198,315,24,118,321,165,161,115,388,282,218,372,54,443,303,108,283,108,217,33,64,440,201,121,96,50,245,12,339,73,174,328,401,96,175,225,350,209,316,490,13,197,385,98,185,11,82,86,168,177,466,149,238,298,95,300,241,271,22,19,339,238,260,458,300,148,204,206,85,102,320,229,70,196,171,152,65,132,200,215,112,159,103,295,49,249,277,205,410,16,510,469,461,453,292,446,279,40,23,233,144,283,58,287,217,172,65,131,441,408,313,299,333,341,209,78,167,30,301,316,264,316,400,95,243,357,406,365,175,211,249,357,165,273,251,299,304,396,347,199,42,27,200,400,383,452,519,18,21,249,99,204,88,100,253,168,205,420,301,251,244,420,288,188,242,318,43,221,204,36,332,226,110,276,160,463,264,180,69,400,236,476,247,240,205,341,288,413,388,21,339,159,420,361,203,197,92,195,358,135,120,61,165,195,497,148,107,34,175,161,164,21,182,52,104,162,80,78,240,196,301,339,196,216,251,261,265,12,22,354,70,268,250,250,14,180,164,293,158,174,227,251,103,11,227,10,29,243,358,79,272,81,87,93,98,162,106,179,269,26,151,147,91,130,60,77,28,148,86,197,101,166,158,411,162,203,147,218,240,190,167,151,121,94,229,98,154,249,90,26,253,223,252,122,172,134,247,156,138,127,185,167,167,208,147,177,145,157,150,151,150,201,139,131,219,12,133,239,259,119,251,193,246,114,160,196,14,140,151,158,62,371,152,141,47,151,71,150,161,11,236,25,142,38,106,149,44,194,235,152,14,188,196,164,115,101,279,68,173,244,254,90,138,215,271,151,21,228,153,177,26,149,347,32,148,98,148,125,131,150,150,83,139,148,55,114,278,104,10,204,216,337,148,102,147,44,56,149,150,116,131,262,86,221,248,249,138,176,148,90,248,278,253,21,218,179,170,38,88,65,246,61,203,165,63,53,146,130,99,21,33,148,51,111,242,49,153,129,247,215,244,317,232,25,365,158,178,289,259,151,200,141,121,25,200,246,240,15,200,163,150,143,29,100,155,151,30,62,200,212,193,150,246,160,177,225,72,245,168,252,111,147,250,141,150,132,168,236,146,220,185,216,102,243,245,124,145,236,41,88,192,331,245,74,217,197,242,142,239,226,149,282,29,406,252,293,281,253,72,163,22,245,175,299,103,227,108,119,403,128,280,23,217,332,203,268,254,221,188,55,200,127,126,175,249,258,347,303,461,226,210,408,266,183,317,132,247,261,250,177,243,318,253,50,52,287,305,338,401,247,64,229,234,93,296,248,262,326,262,294,248,343,361,243,280,211,176,252,234,335,114,246,242,139,309,536,256,293,204,250,294,348,161,233,277,85,353,241,213,205,196,195,268,321,291,376,129,228,341,246,284,192,155,257,154,225,223,260,281,446,230,195,437,238,221,133,150,383,146,295,237,172,295,195,337,195,290,231,177,237,247,242,212,256,328,320,301,326,285,260,201,199,138,306,239,190,293,128,232,149,263,300,506,254,515,518,204,305,231,222,158,243,246,602,358,275,148,386,94,263,223,284,257,505,388,374,391,236,346,233,329,319,187,356,340,161,414,64,273,48,435,345,229,271,176,219,709,433,436,246,249,173,204,116,204,187,258,316,265,243,199,271,40,171,154,224,265,293,352,258,134,314,260,134,208,208,75,242,231,111,138,261,238,203,209,201,284,285,270,126,202,351,239,239,233,182,257,340,334,248,286,330,371,243,192,213,281,191,231,320,303,333,337,249,260,392,273,211,213,79,234,234,493,301,218,231,241,669,232,186,406,361,341,215,296,166,268,240,326,167,303,156,293,394,188,221,201,336,25,346,319,224,163,201,124,362,261,238,140,300,243,288,340,392,266,246,264,286,55,267,241,272,298,261,257,281,248,261,244,297,347,402,299,206,386,260,224,243,244,187,126,239,55,289,49,126,216,204,183,279,382,293,303,318,273,248,15,118,249,439,201,288,300,240,242,191,452,247,266,172,257,242,152,196,295,243,206,252,311,147,170,159,229,267,236,259,227,270,377,256,208,191,225,256,127,194,163,204,242,289,298,21,238,377,340,298,432,245,240,189,59,250,174,243,357,334,168,257,130,233,218,236,245,236,286,215,304,4771,250,231,322,285,224,377,220,238,118,301,112,236,241,109,242,285,177,141,147,343,364,355,219,163,291,157,169,251,236,229,117,160,123,57,223,173,114,72,138,242,154,281,141,203,165,251,115,298,156,241,202,230,136,151,214,204,305,107,143,242,323,206,188,239,210,110,207,191,61,275,172,318,56,419,268,444,268,196,164,192,409,416,222,202,153,253,685,197,60,192,187,260,248,151,183],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"fillcolor\":\"rgba(255,255,255,0)\",\"hoveron\":\"points\",\"hovertemplate\":\"variable=0<br>Article Length (words)=%{x}<extra></extra>\",\"jitter\":0,\"legendgroup\":\"0\",\"line\":{\"color\":\"rgba(255,255,255,0)\"},\"marker\":{\"color\":\"#636efa\",\"symbol\":\"line-ns-open\"},\"name\":\"0\",\"offsetgroup\":\"0\",\"showlegend\":false,\"x\":[165,44,142,252,291,71,276,202,273,187,75,219,251,188,201,193,270,103,188,181,288,82,204,160,142,393,231,147,247,218,42,13,243,39,247,164,156,18,128,111,143,32,227,273,206,154,215,45,182,77,132,246,68,238,327,109,113,197,214,255,211,102,349,275,225,200,275,166,150,61,71,215,237,81,88,95,303,153,91,108,296,292,96,196,29,246,51,148,185,187,295,210,298,246,44,196,244,225,241,171,97,22,55,152,75,204,192,400,147,118,118,80,155,203,184,289,277,329,149,233,155,256,104,247,235,78,41,245,214,250,134,270,102,121,274,142,245,109,196,199,301,22,147,137,234,31,36,305,58,165,17,168,126,181,220,266,44,150,155,202,253,146,291,200,258,209,267,248,163,173,150,252,246,113,150,158,297,243,99,95,75,19,258,236,120,155,242,178,36,245,208,255,136,130,300,151,266,18,248,18,219,205,261,250,223,248,172,262,268,189,439,201,291,148,151,100,224,160,197,184,144,32,69,161,150,207,244,51,240,100,151,35,179,78,131,192,65,158,170,51,373,193,168,241,121,79,358,182,264,176,255,121,190,169,93,145,170,203,100,67,172,229,157,292,234,210,162,168,122,222,192,152,243,70,195,123,49,142,199,151,254,271,88,250,393,261,120,141,50,315,229,300,171,319,191,175,209,84,352,348,381,15,158,137,134,132,235,136,158,28,115,160,203,27,199,123,85,203,226,234,161,215,347,31,205,158,174,90,278,225,152,172,270,265,192,146,268,151,46,218,243,190,200,247,163,180,289,68,263,241,286,244,159,61,21,47,246,179,169,241,143,192,74,66,250,179,227,18,210,161,95,31,252,135,178,235,243,152,148,204,188,116,25,193,128,234,226,348,260,45,84,30,143,217,201,223,261,200,69,165,27,271,206,193,296,13,158,189,187,233,130,143,83,28,102,169,178,233,14,188,153,151,133,293,249,157,39,142,197,78,38,85,155,175,55,242,161,44,345,87,126,257,163,21,261,272,157,120,154,243,151,124,214,221,234,162,242,249,269,202,168,195,196,355,161,240,114,246,255,173,189,248,119,131,147,266,476,432,249,353,246,147,200,242,173,159,128,282,166,154,12,535,13,153,150,172,40,197,217,126,125,148,218,124,147,15,123,185,145,151,174,191,199,155,155,144,211,73,146,121,151,49,178,225,52,150,182,185,154,170,188,268,160,160,329,405,297,244,189,207,145,147,147,220,140,204,258,119,128,163,177,150,201,72,180,245,127,19,74,142,154,168,134,169,348,267,209,194,181,201,193,225,184,229,254,98,169,174,79,565,180,246,120,137,153,52,26,21,105,288,202,251,202,64,109,316,89,90,217,430,42,200,203,298,154,236,236,247,405,194,195,302,244,219,355,200,250,253,170,182,255,261,196,123,30,186,250,376,191,211,371,17,321,233,292,247,203,222,234,299,294,306,245,130,94,260,220,265,342,34,194,99,196,459,248,187,150,285,232,246,111,48,222,241,298,106,283,248,53,149,72,295,239,179,248,219,72,175,220,170,217,270,158,235,289,203,211,204,186,227,250,296,47,190,205,217,341,254,153,176,418,64,151,226,244,188,232,61,273,239,213,143,251,260,530,95,124,301,46,215,118,150,253,200,207,84,118,79,252,199,201,191,498,213,116,233,243,114,137,218,189,195,246,52,56,123,184,200,147,212,58,54,130,266,170,186,90,149,38,75,244,80,49,144,114,166,188,186,238,182,140,151,10,18,98,119,199,192,231,163,133,181,78,126,50,221,37,191,219,237,225,228,195,281,191,177,293,76,199,62,299,134,193,248,33,26,52,256,71,82,287,197,25,194,74,222,253,231,67,217,130,148,43,205,152,198,206,170,197,126,150,274,136,89,151,150,199,10,200,173,228,296,247,302,257,171,87,238,247,261,246,192,26,131,123,308,185,224,303,93,201,252,259,234,222,212,172,53,54,113,315,109,107,377,234,150,204,285,126,246,216,228,297,150,200,329,258,314,48,156,182,364,143,155,236,195,314,198,174,241,280,45,39,139,294,274,218,268,146,177,229,122,142,23,31,346,315,207,215,168,242,215,346,44,26,255,112,368,289,162,96,207,223,197,15,164,261,241,61,297,204,350,102,91,168,436,121,112,269,168,80,253,26,198,196,335,131,115,146,528,135,228,355,197,172,58,127,110,253,102,480,212,243,123,169,217,244,64,281,20,253,231,161,590,241,239,207,173,143,227,105,85,247,148,150,259,179,205,138,205,146,254,198,140,207,33,71,170,155,196,175,146,97,116,52,227,198,187,257,67,188,202,166,147,152,152,161,269,187,191,238,155,211,136,67,185,246,138,148,167,259,153,245,222,67,235,175,194,187,405,126,174,1381,34,262,28,302,165,97,314,221,36,277,215,139,268,141,196,163,516,198,300,222,108,198,361,244,267,227,344,258,224,215,123,204,193,91,340,203,250,178,77,199,74,208,244,314,249,126,220,159,219,195,187,242,208,262,202,246,295,250,170,246,152,234,185,209,21,262,261,144,250,304,110,307,22,387,411,286,15,114,77,250,420,208,257,220,150,52,345,266,225,353,159,164,313,252,269,90,265,245,166,385,238,295,157,226,34,149,255,124,292,146,190,172,237,175,357,71,228,252,300,145,128,277,156,246,243,155,198,432,244,134,344,275,448,271,245,48,149,203,255,187,79,224,44,215,246,189,238,146,391,152,236,333,123,46,295,250,245,89,138,98,150,160,272,236,172,48,421,37,182,143,111,191,274,156,267,63,175,174,91,372,244,59,349,197,30,168,199,146,251,153,269,244,199,72,130,102,123,339,136,216,123,128,246,148,191,121,170,86,224,263,115,238,184,76,252,149,131,20,171,196,135,164,21,282,202,123,67,103,161,108,158,142,251,19,217,211,218,224,11,115,108,359,51,55,160,214,169,22,206,347,122,241,168,241,253,295,419,232,287,153,240,157,292,300,197,286,59,254,214,83,258,258,186,59,215,35,115,203,270,58,130,63,150,38,185,327,236,146,165,199,24,149,159,164,22,243,43,241,85,125,348,322,274,230,11,67,44,244,70,206,236,181,88,302,191,214,295,239,238,184,125,260,221,228,199,192,129,199,46,19,132,162,228,77,29,166,203,156,203,85,181,36,127,334,216,249,245,235,249,95,136,19,251,385,41,49,214,83,123,61,130,143,247,245,104,169,58,12,108,71,73,16,296,265,453,301,247,287,164,240,63,64,72,92,16,153,249,28,33,27,238,127,191,326,251,86,87,24,199,342,240,168,361,400,402,140,41,243,257,51,66,352,45,335,205,160,151,136,236,248,261,21,239,209,217,139,367,313,38,123,252,56,134,44,304,43,169,155,356,171,50,196,24,246,248,59,144,100,279,95,88,200,183,157,196,132,249,247,26,46,267,207,226,11,139,224,377,326,180,204,159,45,79,88,24,192,127,177,13,27,149,271,180,70,43,356,234,259,98,188,292,330,145,252,117,95,198,196,213,167,39,35,70,151,126,201,65,183,366,150,248,193,232,188,165,67,60,281,190,155,266,266,91,218,351,241,15,288,137,225,59,176,202,202,269,13,188,184,182,56,210,225,195,255,261,260,126,70,127,185,200,190,356,49,18,271,180,28,198,63,129,149,51,35,178,141,55,150,10,250,214,183,249,56,251,62,200,231,199,167,229,154,236,203,167,139,297,158,49,231,274,221,121,192,85,88,31,250,188,132,310,250,146,208,210,218,346,336,200,283,79,262,308,176,186,301,98,150,72,86,352,21,12,358,23,41,136,223,270,244,306,201,151,120,195,175,184,146,227,54,200,133,200,241,24,17,31,49,74,253,57,73,256,247,58,55,230,39,198,202,248,351,251,149,198,12,191,198,214,148,208,32,151,195,192,346,142,255,176,112,143,96,64,270,300,451,73,124,236,235,72,115,121,74,39,235,251,69,11,128,162,178,149,22,284,178,116,298,147,300,536,220,207,152,213,159,226,235,20,254,21,283,21,222,117,247,133,36,124,196,295,154,195,186,259,264,236,170,150,211,163,112,256,245,160,234,190,259,137,257,128,150,212,18,121,402,231,146,241,91,275,88,181,18,202,250,310,125,273,294,245,424,172,208,284,36,157,250,304,246,209,175,179,247,340,358,469,384,105,63,270,83,321,399,338,209,324,237,175,463,216,485,84,254,202,264,422,234,84,149,342,475,392,471,213,33,30,200,136,131,258,249,249,65,214,277,31,196,130,230,496,485,615,206,405,356,198,147,212,304,138,368,25,198,164,320,456,181,278,197,188,153,103,353,523,456,285,336,284,42,170,260,239,203,342,406,371,261,348,329,25,125,576,79,545,239,148,451,179,540,337,375,97,195,25,98,288,27,425,47,67,117,444,105,334,353,614,338,558,441,434,188,217,71,107,628,170,324,161,330,41,237,469,28,338,42,24,34,224,408,12,56,296,185,521,107,201,101,92,204,310,231,252,157,39,215,137,223,224,441,201,139,335,238,22,10,159,42,33,166,289,208,372,425,184,207,379,189,344,476,276,51,51,254,349,85,211,326,59,235,105,357,260,14,250,193,91,93,129,433,332,316,402,479,221,162,260,126,129,105,201,146,377,136,117,159,193,245,266,375,398,102,318,109,199,336,290,331,277,285,172,147,78,242,214,119,163,297,382,87,381,27,458,144,313,205,139,389,124,364,318,150,159,66,49,86,166,46,194,228,14,127,42,60,198,315,24,118,321,165,161,115,388,282,218,372,54,443,303,108,283,108,217,33,64,440,201,121,96,50,245,12,339,73,174,328,401,96,175,225,350,209,316,490,13,197,385,98,185,11,82,86,168,177,466,149,238,298,95,300,241,271,22,19,339,238,260,458,300,148,204,206,85,102,320,229,70,196,171,152,65,132,200,215,112,159,103,295,49,249,277,205,410,16,510,469,461,453,292,446,279,40,23,233,144,283,58,287,217,172,65,131,441,408,313,299,333,341,209,78,167,30,301,316,264,316,400,95,243,357,406,365,175,211,249,357,165,273,251,299,304,396,347,199,42,27,200,400,383,452,519,18,21,249,99,204,88,100,253,168,205,420,301,251,244,420,288,188,242,318,43,221,204,36,332,226,110,276,160,463,264,180,69,400,236,476,247,240,205,341,288,413,388,21,339,159,420,361,203,197,92,195,358,135,120,61,165,195,497,148,107,34,175,161,164,21,182,52,104,162,80,78,240,196,301,339,196,216,251,261,265,12,22,354,70,268,250,250,14,180,164,293,158,174,227,251,103,11,227,10,29,243,358,79,272,81,87,93,98,162,106,179,269,26,151,147,91,130,60,77,28,148,86,197,101,166,158,411,162,203,147,218,240,190,167,151,121,94,229,98,154,249,90,26,253,223,252,122,172,134,247,156,138,127,185,167,167,208,147,177,145,157,150,151,150,201,139,131,219,12,133,239,259,119,251,193,246,114,160,196,14,140,151,158,62,371,152,141,47,151,71,150,161,11,236,25,142,38,106,149,44,194,235,152,14,188,196,164,115,101,279,68,173,244,254,90,138,215,271,151,21,228,153,177,26,149,347,32,148,98,148,125,131,150,150,83,139,148,55,114,278,104,10,204,216,337,148,102,147,44,56,149,150,116,131,262,86,221,248,249,138,176,148,90,248,278,253,21,218,179,170,38,88,65,246,61,203,165,63,53,146,130,99,21,33,148,51,111,242,49,153,129,247,215,244,317,232,25,365,158,178,289,259,151,200,141,121,25,200,246,240,15,200,163,150,143,29,100,155,151,30,62,200,212,193,150,246,160,177,225,72,245,168,252,111,147,250,141,150,132,168,236,146,220,185,216,102,243,245,124,145,236,41,88,192,331,245,74,217,197,242,142,239,226,149,282,29,406,252,293,281,253,72,163,22,245,175,299,103,227,108,119,403,128,280,23,217,332,203,268,254,221,188,55,200,127,126,175,249,258,347,303,461,226,210,408,266,183,317,132,247,261,250,177,243,318,253,50,52,287,305,338,401,247,64,229,234,93,296,248,262,326,262,294,248,343,361,243,280,211,176,252,234,335,114,246,242,139,309,536,256,293,204,250,294,348,161,233,277,85,353,241,213,205,196,195,268,321,291,376,129,228,341,246,284,192,155,257,154,225,223,260,281,446,230,195,437,238,221,133,150,383,146,295,237,172,295,195,337,195,290,231,177,237,247,242,212,256,328,320,301,326,285,260,201,199,138,306,239,190,293,128,232,149,263,300,506,254,515,518,204,305,231,222,158,243,246,602,358,275,148,386,94,263,223,284,257,505,388,374,391,236,346,233,329,319,187,356,340,161,414,64,273,48,435,345,229,271,176,219,709,433,436,246,249,173,204,116,204,187,258,316,265,243,199,271,40,171,154,224,265,293,352,258,134,314,260,134,208,208,75,242,231,111,138,261,238,203,209,201,284,285,270,126,202,351,239,239,233,182,257,340,334,248,286,330,371,243,192,213,281,191,231,320,303,333,337,249,260,392,273,211,213,79,234,234,493,301,218,231,241,669,232,186,406,361,341,215,296,166,268,240,326,167,303,156,293,394,188,221,201,336,25,346,319,224,163,201,124,362,261,238,140,300,243,288,340,392,266,246,264,286,55,267,241,272,298,261,257,281,248,261,244,297,347,402,299,206,386,260,224,243,244,187,126,239,55,289,49,126,216,204,183,279,382,293,303,318,273,248,15,118,249,439,201,288,300,240,242,191,452,247,266,172,257,242,152,196,295,243,206,252,311,147,170,159,229,267,236,259,227,270,377,256,208,191,225,256,127,194,163,204,242,289,298,21,238,377,340,298,432,245,240,189,59,250,174,243,357,334,168,257,130,233,218,236,245,236,286,215,304,4771,250,231,322,285,224,377,220,238,118,301,112,236,241,109,242,285,177,141,147,343,364,355,219,163,291,157,169,251,236,229,117,160,123,57,223,173,114,72,138,242,154,281,141,203,165,251,115,298,156,241,202,230,136,151,214,204,305,107,143,242,323,206,188,239,210,110,207,191,61,275,172,318,56,419,268,444,268,196,164,192,409,416,222,202,153,253,685,197,60,192,187,260,248,151,183],\"xaxis\":\"x2\",\"yaxis\":\"y2\",\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Article Length (words)\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,0.7326],\"title\":{\"text\":\"count\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0],\"matches\":\"x\",\"showticklabels\":false,\"showgrid\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7426,1.0],\"matches\":\"y2\",\"showticklabels\":false,\"showline\":false,\"ticks\":\"\",\"showgrid\":false},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0a77e2df-b55f-4f59-9475-0c456abd3660');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "abstracts = [len(x.split()) for x in df[\"abstract\"]]\n",
    "px.histogram(abstracts, nbins=400, marginal=\"rug\", labels={\"value\":\"Article Length (words)\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize then lock and load into training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize the data I defined a generic tokenization function, and then I applied this function to all the samples by using map(). Inside the tokenization function, I used the tokenizer imported in the beginning.\n",
    "\n",
    "The tokenizer has some important parameters to set:\n",
    "\n",
    "column to tokenize. In this case “abstract”.\n",
    "padding. In this case = “max_lenght” to pad a sequence to the maximum length specified by the max_length parameter.\n",
    "truncation. If true, truncates sequences longer than the maximum length, specified by the max_length parameter.\n",
    "max_length. Specifies the maximum length of a sequence.\n",
    "Please note that by default the map() method sends batches of 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The tokenization function\n",
    "def tokenization(data):\n",
    "    tokens = tokenizer(data, padding=\"max_length\", truncation=True, max_length=300)\n",
    "    return tokens\n",
    "train_gpt = train_gpt.reset_index(drop=True)\n",
    "test_gpt = test_gpt.reset_index(drop=True)\n",
    "train = train_gpt[['abstract']]\n",
    "val = test_gpt[['abstract']]\n",
    "# Apply the tokenizer in batch mode and drop all the columns except the tokenization result\n",
    "train_token = train_gpt['abstract'].map(tokenization)\n",
    "val_token = test_gpt['abstract'].map(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(text):\n",
    "    text[\"labels\"] = text[\"input_ids\"].copy()\n",
    "    return text\n",
    "\n",
    "# Add the labels column using map()\n",
    "lm_train = train_token.map(create_labels)\n",
    "lm_val = val_token.map(create_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices(dict(lm_train))\n",
    "validation_set = tf.data.Dataset.from_tensor_slices(dict(lm_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.batch(16)\n",
    "train_set = train_set.shuffle(42)\n",
    "validation_set = validation_set.batch(16)\n",
    "validation_set = validation_set.shuffle(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the learning rate scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0005,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.95,\n",
    "    staircase=False)\n",
    "    \n",
    "# Exponential decay learning rate\n",
    "optimizer = AdamWeightDecay(learning_rate=lr_schedule, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 81912576  \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,912,576\n",
      "Trainable params: 81,912,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is optional\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "model_name = \"GPT-2_PubMed\"\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-papers\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8551131c674c698d4e374206e92755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit with callbacks\n",
    "#model.fit(lm_train, validation_data=validation_set, epochs=1, workers=9, use_multiprocessing=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('data/publications/final_database_of_papers.xlsx',index_col=0)\n",
    "train_test_ratio = 0.9\n",
    "train_valid_ratio = 7/9\n",
    "df_full_train, df_test = train_test_split(df, train_size = train_test_ratio, random_state = 1)\n",
    "df_train, df_valid = train_test_split(df_full_train, train_size = train_valid_ratio, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(df, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    summaries = df['abstract'].tolist()\n",
    "    for summary in summaries:\n",
    "        summary = str(summary).strip()\n",
    "        summary = re.sub(r\"\\s\", \" \", summary)\n",
    "        bos_token = '<BOS>'\n",
    "        eos_token = '<EOS>'\n",
    "        data += bos_token + ' ' + summary + ' ' + eos_token + '\\n'\n",
    "        \n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dataset(df_train, 'train.txt')\n",
    "build_dataset(df_valid, 'valid.txt')\n",
    "build_dataset(df_test, 'test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
